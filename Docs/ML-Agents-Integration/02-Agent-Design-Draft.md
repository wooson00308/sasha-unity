# 2. 에이전트 설계 초안

ML-Agents 파일럿 AI의 관측, 행동, 보상에 대한 초기 설계안.

## 1. 관측 (Observations)

에이전트가 의사결정을 위해 사용하는 정보 벡터 (값은 0~1 정규화 가정).

*   **자신(Self) 상태:**
    *   `Body` 파츠 내구도 비율 (1 float)
    *   현재 AP 비율 (1 float)
    *   장착 무기별 상태 (최대 N개, 각 무기당 3 floats: 작동 여부, 탄약 비율, 재장전 여부)
    *   방어 중 여부 (1 float: 0 or 1)
    *   이동 가능 여부 (1 float: 0 or 1)
*   **가장 가까운 적(Closest Enemy) 상태:**
    *   상대적 거리 (1 float)
    *   상대적 방향 벡터 (2 floats: X, Z)
    *   적이 나를 타겟팅하는지 여부 (1 float: 0 or 1)
    *   적 `Body` 내구도 비율 (1 float)
    *   적 작동 가능 여부 (1 float: 0 or 1)
*   **가장 가까운 손상된 아군(Closest Damaged Ally) 상태:** (필요시)
    *   상대적 거리 (1 float)
    *   상대적 방향 벡터 (2 floats: X, Z)
    *   아군 `Body` 내구도 비율 (1 float)
    *   아군 작동 가능 여부 (1 float: 0 or 1)

*   **참고:** 벡터 크기 고정 필요 (예: 무기 슬롯 최대 개수 가정). 정규화 방식 구체화 필요.

## 2. 행동 (Actions)

이산적(Discrete) 행동 공간 사용. 에이전트는 주 행동 유형만 결정.

*   **Branch 0: 주 행동 선택 (Size 7)**
    *   `0`: 대기 (Wait/Idle)
    *   `1`: 공격 (Attack - 대상은 룰 기반 선택)
    *   `2`: 이동 (Move - 목표 위치는 룰 기반 선택)
    *   `3`: 방어 (Defend)
    *   `4`: 재장전 (Reload - 대상 무기는 룰 기반 선택)
    *   `5`: 아군 수리 (Repair Ally - 대상 아군은 룰 기반 선택)
    *   `6`: 자가 수리 (Repair Self)

*   **참고:** 세부 목표(공격 대상, 이동 위치 등) 결정 로직은 `MLAgentBehaviorStrategy` 래퍼 클래스에서 현재 게임 상태를 바탕으로 처리하여 에이전트의 부담을 줄임.
*   **결과 랜덤성:** AI 에이전트가 행동을 '결정'하더라도, 해당 행동의 실행 '결과'(예: 공격 명중 여부, 크리티컬 발생 여부, 피격 부위 등)는 기존 전투 시스템의 확률적 요소에 따라 결정됨. 에이전트는 이러한 결과의 불확실성을 감안하여 최적의 행동을 선택하도록 학습해야 함.

## 3. 보상 (Rewards)

행동의 결과에 따라 주어지는 피드백.

*   **긍정적 보상 (+):**
    *   적 데미지 입힘: + (데미지량 비례?)
    *   적 파츠 파괴: ++
    *   적 기체 파괴: +++
    *   공격 회피 성공: +
    *   아군 수리 성공: +
    *   자가 수리 성공: +
    *   전투 승리 (종료 시): ++++
*   **부정적 보상 (-):**
    *   자신 데미지 입음: - (데미지량 비례?)
    *   자신 파츠 파괴: --
    *   자신 기체 파괴 / 전투 패배 (종료 시): ---
    *   비효율적 행동 시도 페널티: -
    *   시간 경과 페널티 (Living Penalty): - (매 스텝마다 약간의 감점)

*   **참고:** 보상 값의 스케일링 및 구체적인 계산 방식은 실험을 통해 조정 필요. 